{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himanshushakyawar/Python_AI_ML_NIT/blob/main/Gradient_Boosting_B42.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn-ue9bbFTbe"
      },
      "source": [
        "#**Demo: Gradient Boosting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJQKpsEpFsWO"
      },
      "source": [
        "###**Problem Definition**\n",
        "\n",
        "Perform Gradient Boosting on the sklearn's Digits dataset and find the accuracy. We will create different machine learning models and compare the performance with Gradient Boosting Classifier\n",
        "\n",
        "  \n",
        "\n",
        "###**Dataset Description**\n",
        "\n",
        "This dataset is made up of 1797 8x8 images. Each image, like the one shown below, is of a hand-written digit. In order to utilize an 8x8 figure like this, weâ€™d have to first transform it into a feature vector with length 64.\n",
        "\n",
        "\n",
        "###**Tasks to be performed**\n",
        "\n",
        "*   Understanding Gradient Boosting\n",
        "*   Importing the libraries\n",
        "*   Load the dataset\n",
        "*   Split the Dataset\n",
        "*   Implement different models and evaluating the performance of the model\n",
        "*   Implement GradientBoostingClassifier using sklearn library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-riiAvSJ-Ft"
      },
      "source": [
        "##Understanding Gradient Boosting\n",
        "Gradient Boosting is an ensemble method, meaning it uses more than one model to perform a specific task. It uses a loss function which needs to be optimised on the additive models, it means while adding trees a gradient descent method is used to minimize the loss.\n",
        "\n",
        "This method is same as AdaBoost, the only difference is it uses the loss function for boosting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRtqR3uGMaGz"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwPf5gWxIPVw"
      },
      "source": [
        "##Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_RLpM_vEOVq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold,cross_val_score\n",
        "#ML algorithm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opJLPFpAIN6G"
      },
      "source": [
        "##Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbBwdNeHFmxB"
      },
      "outputs": [],
      "source": [
        "digits = load_digits()\n",
        "X=digits.data\n",
        "y=digits.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK8BPktuJj-D"
      },
      "source": [
        "##Split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5a_1XWyJi2I"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMTdSwflJ2F4"
      },
      "source": [
        "##Implement different models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRJj72uIJ0a2",
        "outputId": "ea6e4562-8147-4468-8ad5-1bbbe545d980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decision tree : acc: 0.8368952380952381(standard deviation: 0.01616584369569217)\n",
            "svm : acc: 0.40100317460317464(standard deviation: 0.03429328311517924)\n",
            "naive bayes : acc: 0.8440825396825398(standard deviation: 0.03482816096451723)\n"
          ]
        }
      ],
      "source": [
        "models=[]\n",
        "models.append(('decision tree',DecisionTreeClassifier()))\n",
        "models.append(('svm',SVC(gamma='auto')))\n",
        "models.append(('naive bayes',GaussianNB()))\n",
        "\n",
        "\n",
        "for name,model in models:\n",
        "  cross_val_sc=cross_val_score(model,X_train,y_train,scoring='accuracy',cv=10)\n",
        "  print('{} : acc: {}(standard deviation: {})'.format(name,cross_val_sc.mean(),cross_val_sc.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe3YOGsaMm0A"
      },
      "source": [
        "We can see a single decision tree is giving us quite good accuracy i.e. nearly 85%.\n",
        "Let us see how gradient boosting will perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUKphyQuNiut"
      },
      "source": [
        "##Implement Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7IGfwe0MOQJ",
        "outputId": "2ca47972-46ed-4eb6-aa10-1af74b076973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9685185185185186\n"
          ]
        }
      ],
      "source": [
        "gbc=GradientBoostingClassifier(n_estimators=200,learning_rate=0.1)\n",
        "gbc.fit(X_train,y_train)\n",
        "y_pred1=gbc.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijUne0We4rj-"
      },
      "source": [
        "We can see, the above gradient boosting algorithm is using 200 decision trees with a learning rate of 0.1, improved the accuracy to 96%."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Gradient_Boosting - B42.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}